{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdaa8978-0843-4e7c-9914-2518a0c7c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import scipy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a62ab3b-b0e0-495b-9388-bcb7507d1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': '8842281e1d1347389f2ab93d60773d4d', 'timestamp': '2017-08-30', 'review_sentences': [[0, 'This is a special book.'], [0, 'It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind.'], [0, 'This is what I love about good science fiction - it pushes your thinking about where things can go.'], [0, \"It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I've read.\"], [0, 'For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc.'], [0, 'It is a book about science, and aliens.'], [0, 'The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell.'], [1, 'Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though.'], [1, 'But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned?'], [1, 'That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together.'], [1, 'I did like how the book unveiled the Trisolaran culture through the game.'], [1, \"It was a smart way to build empathy with them and also understand what they've gone through across so many centuries.\"], [1, 'And who know a 3 body problem was an unsolvable math problem?'], [1, \"But I still don't get who made the game - maybe that will come in the next book.\"], [1, 'I loved this quote:'], [1, '\"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists?'], [1, 'How many neutrons and electrons?'], [1, 'Probably no fewer than a hundred million.'], [1, 'Every collision was probably the end of the civilizations and intelligences in a microcosmos.'], [1, 'In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons.'], [1, 'Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\"']], 'rating': 5, 'has_spoiler': True, 'book_id': '18245960', 'review_id': 'dfdbb7b0eb5a7e4c26d59a937e2e5feb'}\n"
     ]
    }
   ],
   "source": [
    "# Path to the .json.gz file\n",
    "file_path = 'goodreads_reviews_spoiler.json.gz'\n",
    "ct = 0\n",
    "# Open and read the file line by line\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        # Parse each line as a separate JSON object\n",
    "        try:\n",
    "            ct += 1\n",
    "            review = json.loads(line)\n",
    "            #review.pop('has_spoiler', None)\n",
    "            data.append(review)\n",
    "            if ct == 102000:\n",
    "                break\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {e}\")\n",
    "\n",
    "# Example: Print the first parsed review\n",
    "if data:\n",
    "    print(data[0])\n",
    "else:\n",
    "    print(\"No data was parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1aba91-04e0-4bce-a3cd-cd63d974c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 101750\n",
    "test_size = 250\n",
    "\n",
    "ones_minus_zeros = 0\n",
    "ratingsTrain = []\n",
    "for i in data[:train_size]:\n",
    "    if i['has_spoiler']:\n",
    "        copy_to_upload = i\n",
    "        ones = sum([sent[0] for sent in copy_to_upload['review_sentences']])\n",
    "        non_spoilers = [sentence for sentence in i['review_sentences'] if sentence[0] == 0]\n",
    "        spoilers = [sentence for sentence in i['review_sentences'] if sentence[0] == 1]\n",
    "        selected_non_spoilers = []\n",
    "        if ones == 1:\n",
    "            #ones_minus_zeros -= 2\n",
    "            k = min(3, len(non_spoilers))\n",
    "            selected_non_spoilers = random.choices(non_spoilers, k=k)\n",
    "            ones_minus_zeros -= (k-1)\n",
    "        elif ones == 2:\n",
    "            k = min(2, len(non_spoilers))\n",
    "            selected_non_spoilers = random.choices(non_spoilers, k=k)\n",
    "            ones_minus_zeros -= (k-2)\n",
    "        elif ones == 3:\n",
    "            k = min(1, len(non_spoilers))\n",
    "            selected_non_spoilers = random.choices(non_spoilers, k=k)\n",
    "            ones_minus_zeros += (3 - k)\n",
    "\n",
    "        else:\n",
    "            ones_minus_zeros += len(spoilers)\n",
    "            #selected_non_spoilers = random.choices(non_spoilers, k=1)\n",
    "            \n",
    "        new_sentences = selected_non_spoilers + spoilers\n",
    "        copy_to_upload['review_sentences'] = new_sentences\n",
    "\n",
    "        ratingsTrain.append(copy_to_upload)\n",
    "    elif ones_minus_zeros > 3:\n",
    "        copy_to_upload = i\n",
    "        if (ones_minus_zeros // 2) < len(i['review_sentences']):\n",
    "            selected_non_spoilers = random.choices(i['review_sentences'], k=ones_minus_zeros // 2)\n",
    "            ones_minus_zeros -= (ones_minus_zeros // 2)\n",
    "        else:\n",
    "            selected_non_spoilers = i['review_sentences']\n",
    "            ones_minus_zeros -= len(i['review_sentences'])\n",
    "        copy_to_upload['review_sentences'] = selected_non_spoilers\n",
    "        ratingsTrain.append(copy_to_upload)\n",
    "\n",
    "ratingsTest = data[train_size:train_size + test_size]\n",
    "tot_sent = 0\n",
    "for i in ratingsTrain:\n",
    "    tot_sent += len(i['review_sentences'])\n",
    "print(tot_sent)\n",
    "tot_sent = 0\n",
    "for i in ratingsTest:\n",
    "    tot_sent += len(i['review_sentences'])\n",
    "tot_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295db4f0-7325-4e15-9e0f-86c7b649ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "reviewsPerUser = defaultdict(list)\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "\n",
    "for d in data:\n",
    "    usersPerItem[d['book_id']].add(d['user_id'])\n",
    "    itemsPerUser[d['user_id']].add(d['book_id'])\n",
    "    ratingsPerUser[d['user_id']].append(d['rating'])\n",
    "    ratingsPerItem[d['book_id']].append(d['rating'])\n",
    "    for i in d['review_sentences']:\n",
    "        reviewsPerItem[d['book_id']].append(i[0])\n",
    "        reviewsPerUser[d['user_id']].append(i[0])\n",
    "\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for d in data:\n",
    "    bookCount[d['book_id']] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "PopularityBooks = [x[-1] for x in mostPopular]\n",
    "\n",
    "mostPopularUsers = [[len(value), key] for key, value in itemsPerUser.items()]\n",
    "mostPopularUsers.sort()\n",
    "mostPopularUsers.reverse()\n",
    "PopularityUsers = [x[-1] for x in mostPopularUsers]\n",
    "\n",
    "userIDs, itemIDs = {}, {}\n",
    "for d in data:\n",
    "    if d['user_id'] not in userIDs: userIDs[d['user_id']] = len(userIDs)\n",
    "    if d['book_id'] not in itemIDs: itemIDs[d['book_id']] = len(itemIDs)\n",
    "\n",
    "nUsers, nItems = len(userIDs), len(itemIDs)\n",
    "Xui = scipy.sparse.lil_matrix((nUsers, nItems))\n",
    "for d in data:\n",
    "    Xui[userIDs[d['user_id']], itemIDs[d['book_id']]] = 1 \n",
    "\n",
    "Xui_csr = scipy.sparse.csr_matrix(Xui)\n",
    "\n",
    "book_popularity = np.sum(Xui_csr, axis=0).A1\n",
    "books_read_by_user = Xui_csr.sum(axis=1).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753eacbb-1bd0-4f3d-b0e6-3fa26ed5382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_book_pop = sum(book_popularity) / len(book_popularity)\n",
    "avg_user_freq = sum(books_read_by_user) / len(books_read_by_user)\n",
    "avg_pop_by_freq = avg_book_pop * avg_user_freq\n",
    "avg_book_rank = len(PopularityBooks) / 2\n",
    "avg_user_rank = len(PopularityUsers) / 2\n",
    "#avg_ct_book_spoilers = 0.179359243697479\n",
    "spoiler_flat = [sum(sublist) for sublist in reviewsPerItem.values()]\n",
    "avg_ct_book_spoilers = sum(spoiler_flat) / len(spoiler_flat) if spoiler_flat else 0\n",
    "spoiler_flat_2 = [sum(sublist) for sublist in reviewsPerUser.values()]\n",
    "#avg_ct_user_spoilers = 10.841269841269842\n",
    "avg_ct_user_spoilers = sum(spoiler_flat_2) / len(spoiler_flat_2) if spoiler_flat_2 else 0\n",
    "flattened_1 = [item for sublist in ratingsPerItem.values() for item in sublist]\n",
    "ovr_avg_rating_item =  sum(flattened_1) / len(flattened_1) if flattened_1 else 0\n",
    "flattened_2 = [item for sublist in ratingsPerUser.values() for item in sublist]\n",
    "ovr_avg_rating_user = sum(flattened_2) / len(flattened_2) if flattened_2 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde37dd9-10b6-4fa0-8029-f609de22da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start embeddings\n",
      "end embeddings\n"
     ]
    }
   ],
   "source": [
    "users = set(userIDs.keys())\n",
    "books = set(itemIDs.keys())\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def get_bert_embeddings(sentences):\n",
    "    \"\"\"Generate BERT embeddings for a list of sentences.\"\"\"\n",
    "    tokens = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        # Use the [CLS] token's embedding for each sentence\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embeddings.numpy()\n",
    "\n",
    "# Precompute embeddings for all sentences\n",
    "def precompute_embeddings(data):\n",
    "    all_sentences = [sentence[1] for d in data for sentence in d['review_sentences']]\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(all_sentences), batch_size):\n",
    "        batch = all_sentences[i:i + batch_size]\n",
    "        batch_embeddings = get_bert_embeddings(batch)\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    return np.vstack(all_embeddings)\n",
    "print('start embeddings')\n",
    "# Precompute embeddings for the training set\n",
    "train_embeddings = precompute_embeddings(ratingsTrain + ratingsTest)\n",
    "#np.save('train_embeddings.npy', train_embeddings)  # Save embeddings for later use\n",
    "print('end embeddings')\n",
    "# Load precomputed embeddings\n",
    "#train_embeddings = np.load('train_embeddings.npy')\n",
    "\n",
    "# Update create_vector to use precomputed BERT embeddings\n",
    "def create_vector(review):\n",
    "    global users, books, avg_ct_book_spoilers, avg_ct_user_spoilers, ovr_avg_rating_item, ovr_avg_rating_user\n",
    "\n",
    "    u = review['user_id']\n",
    "    b = review['book_id']\n",
    "    timestamp = review['timestamp']\n",
    "    sentence = review['BERT_data']\n",
    "    rating = review['rating']\n",
    "\n",
    "    # Default feature values\n",
    "    book_in = b in books\n",
    "    user_in = u in users\n",
    "\n",
    "    if len(sentence) == 0:\n",
    "        sentence = np.zeros(768)  # Placeholder for BERT embedding\n",
    "    temporal_feature = 0  # Placeholder for temporal information\n",
    "    user_rating = rating if rating is not None else 0  # Use given rating\n",
    "\n",
    "    # Book and user interaction features\n",
    "    if book_in and user_in:\n",
    "        book_pop = book_popularity[itemIDs[b]]\n",
    "        user_freq = books_read_by_user[userIDs[u]]\n",
    "        pop_by_freq = book_pop * user_freq\n",
    "        book_rank = PopularityBooks.index(b)\n",
    "        user_rank = PopularityUsers.index(u)\n",
    "        ct_book_spoilers = sum(reviewsPerItem[b])\n",
    "        ct_user_spoilers = sum(reviewsPerUser[u])\n",
    "        avg_rating_item = ratingsPerItem[b]\n",
    "        avg_rating_item = sum(avg_rating_item) / len(avg_rating_item)\n",
    "        avg_rating_user = ratingsPerUser[u]\n",
    "        avg_rating_user = sum(avg_rating_user) / len(avg_rating_user)\n",
    "    elif not user_in and not book_in:\n",
    "        book_pop = len(books)\n",
    "        user_freq = 0\n",
    "        pop_by_freq = 0\n",
    "        book_rank = len(books)\n",
    "        user_rank = len(users)\n",
    "        ct_book_spoilers = avg_ct_book_spoilers\n",
    "        ct_user_spoilers = avg_ct_user_spoilers\n",
    "        avg_rating_item = ovr_avg_rating_item\n",
    "        avg_rating_user = ovr_avg_rating_user\n",
    "    elif not user_in:\n",
    "        book_pop = book_popularity[itemIDs[b]]\n",
    "        user_freq = 0\n",
    "        pop_by_freq = book_pop\n",
    "        book_rank = PopularityBooks.index(b)\n",
    "        user_rank = len(PopularityUsers)\n",
    "        ct_book_spoilers = sum(reviewsPerItem[b])\n",
    "        ct_user_spoilers = avg_ct_user_spoilers\n",
    "        avg_rating_item = ratingsPerItem[b]\n",
    "        avg_rating_item = sum(avg_rating_item) / len(avg_rating_item)\n",
    "        avg_rating_user = ovr_avg_rating_user\n",
    "    else:\n",
    "        book_pop = 0\n",
    "        user_freq = books_read_by_user[userIDs[u]]\n",
    "        pop_by_freq = user_freq\n",
    "        book_rank = len(PopularityBooks)\n",
    "        user_rank = PopularityUsers.index(u)\n",
    "        ct_book_spoilers = avg_ct_book_spoilers\n",
    "        ct_user_spoilers = sum(reviewsPerUser[u])\n",
    "        avg_rating_item = ovr_avg_rating_item\n",
    "        avg_rating_user = ratingsPerUser[u]\n",
    "        avg_rating_user = sum(avg_rating_user) / len(avg_rating_user)\n",
    "\n",
    "    # Use precomputed BERT embedding for the sentence\n",
    "    #bert_embedding = get_bert_embedding(sentence)  # Assuming the embedding was precomputed\n",
    "\n",
    "    # Add temporal feature based on timestamp\n",
    "    try:\n",
    "        review_date = datetime.strptime(timestamp, \"%Y-%m-%d\")\n",
    "        current_date = datetime.now()\n",
    "        temporal_feature = (current_date - review_date).days  # Time in days\n",
    "    except ValueError:\n",
    "        temporal_feature = 0  # Default value if timestamp parsing fails\n",
    "\n",
    "    # Combine features\n",
    "    vals = [\n",
    "        1,  # Bias term\n",
    "        book_pop,  # Book popularity\n",
    "        pop_by_freq,  # Popularity scaled by user frequency\n",
    "        user_rank,  # User ranking\n",
    "        user_rating,  # User's rating\n",
    "        temporal_feature,  # Temporal feature\n",
    "        ct_book_spoilers,\n",
    "        ct_user_spoilers,\n",
    "        avg_rating_item,\n",
    "        avg_rating_user\n",
    "    ]\n",
    "\n",
    "    # Append BERT embeddings\n",
    "    vals.extend(sentence.tolist())\n",
    "    \n",
    "    return vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a42431-f463-4d9a-9525-0a401704176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation feature vectors using parallelization\n",
    "x_train_pre = []\n",
    "y_train = []\n",
    "embedding_index = 0\n",
    "for d in ratingsTrain:\n",
    "    filtered_data = {key: value for key, value in d.items() if key != 'review_sentences' and key != 'has_spoiler'}\n",
    "    for sentence in d['review_sentences']:\n",
    "        bert_embedding = train_embeddings[embedding_index]  # Pop the precomputed embedding for the sentence\n",
    "        filtered_data['BERT_data'] = bert_embedding\n",
    "        x_train_pre.append(filtered_data)\n",
    "        y_train.append(sentence[0])\n",
    "        embedding_index += 1\n",
    "\n",
    "x_valid_pre = []\n",
    "y_valid = []\n",
    "for d in ratingsTest:\n",
    "    filtered_data = {key: value for key, value in d.items() if key != 'review_sentences' and key != 'has_spoiler'}\n",
    "    for sentence in d['review_sentences']:\n",
    "        bert_embedding = train_embeddings[embedding_index]  # Pop the precomputed embedding for the sentence\n",
    "        filtered_data['BERT_data'] = bert_embedding\n",
    "        x_valid_pre.append(filtered_data)\n",
    "        y_valid.append(sentence[0])\n",
    "        embedding_index += 1\n",
    "        \n",
    "# Parallelize vector creation for training and validation data\n",
    "x_train = [create_vector(i) for i in x_train_pre]\n",
    "x_valid = [create_vector(i) for i in x_valid_pre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbbac36e-b476-4eaa-a52c-7079c8c8eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  100\n",
      "recall error, tp + fn = 0\n",
      "accuracy:  0.9863945578231292\n",
      "tp:  0 tn:  145 fp:  2 fn:  0\n",
      "F1 error, precision + recall = 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m     curr_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC: \u001b[39m\u001b[38;5;124m\"\u001b[39m, c)\n\u001b[0;32m---> 69\u001b[0m \u001b[43mget_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst spoiler position: \u001b[39m\u001b[38;5;124m\"\u001b[39m, first_one)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast spoiler position: \u001b[39m\u001b[38;5;124m\"\u001b[39m, last_one)\n",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36mget_metric\u001b[0;34m(y_true, pred, scores)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (precision \u001b[38;5;241m*\u001b[39m recall) \u001b[38;5;241m/\u001b[39m (precision \u001b[38;5;241m+\u001b[39m recall))\n\u001b[0;32m---> 34\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:640\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    638\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[1;32m    639\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    649\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    650\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    654\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "def get_metric(y_true, pred, scores):\n",
    "    corr = 0\n",
    "    tn, tp, fn, fp = 0,0,0,0\n",
    "    for guess, actual in zip(pred, y_true):\n",
    "        if guess == actual:\n",
    "            corr += 1\n",
    "            if guess == 0:\n",
    "                tn += 1\n",
    "            else:\n",
    "                tp += 1\n",
    "        else:\n",
    "            if guess == 0:\n",
    "                fn += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    if tp + fp == 0:\n",
    "        precision = 0\n",
    "        print(\"precision error, tp + fp = 0\")\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    if tp + fn == 0:\n",
    "        recall = 0\n",
    "        print(\"recall error, tp + fn = 0\")\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    print(\"accuracy: \", corr / len(pred))\n",
    "    print(\"tp: \", tp, \"tn: \", tn, \"fp: \", fp, \"fn: \", fn)\n",
    "    # Calculate F1 Score\n",
    "    if precision + recall == 0:\n",
    "        print(\"F1 error, precision + recall = 0\")\n",
    "    else:\n",
    "        print(\"F1: \", 2 * (precision * recall) / (precision + recall))\n",
    "    \n",
    "    auc = roc_auc_score(y_valid, scores)\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "for c in [100]:\n",
    "    mod = linear_model.LogisticRegression(C=c, class_weight = 'balanced')\n",
    "    mod.fit(x_train,y_train)\n",
    "    scores = mod.decision_function(x_valid)\n",
    "    new_results = {index: score for index, score in enumerate(scores)}     \n",
    "    sorted_items = sorted(new_results.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    pred = np.zeros(len(x_valid))\n",
    "    if len(x_valid) % 2 == 1:\n",
    "        for i in sorted_items[:int(1.1 * len(x_valid)//100) + 1]:\n",
    "            pred[i[0]] = 1\n",
    "    else:\n",
    "        for i in sorted_items[:int(1.1 * len(x_valid)//100)]:\n",
    "            pred[i[0]] = 1\n",
    "    \n",
    "    curr_counter = 0\n",
    "    all_pos = 0\n",
    "    first_one = 0\n",
    "    last_one = 0\n",
    "    got_one = False\n",
    "    got_all = 0\n",
    "    while got_one < sum(y_valid) and curr_counter < len(sorted_items):\n",
    "        if y_valid[sorted_items[curr_counter][0]] == 1:\n",
    "            got_all += 1\n",
    "            if got_all == sum(y_valid):\n",
    "                last_one = curr_counter\n",
    "            all_pos += curr_counter\n",
    "            if got_one == False:\n",
    "                got_one = True \n",
    "                first_one = curr_counter\n",
    "        curr_counter += 1\n",
    "    print(\"C: \", c)\n",
    "    get_metric(y_valid, pred, mod.predict_proba(x_valid)[:,1])\n",
    "    print(\"First spoiler position: \", first_one)\n",
    "    print(\"Last spoiler position: \", last_one)\n",
    "    print(\"Average spoiler position: \", all_pos / sum(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcdba0-7618-4151-bc1b-add8fc70db71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
