{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970e543d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To upload to github, all changes you make to a clone of this repo / notebook should be stored somewhere and you can just push in a terminal or github desktop'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To upload to github, all changes you make to a clone of this repo / notebook \n",
    "should be stored somewhere and you can just push in a terminal or github desktop'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Everything here is made for the bag of words model he showed in Tuesday Nov 19 lecture \n",
    "- feel free to change anything, make your own model here or in a different ipynb'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bce83e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': '8842281e1d1347389f2ab93d60773d4d', 'timestamp': '2017-08-30', 'review_sentences': [[0, 'This is a special book.'], [0, 'It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind.'], [0, 'This is what I love about good science fiction - it pushes your thinking about where things can go.'], [0, \"It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I've read.\"], [0, 'For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc.'], [0, 'It is a book about science, and aliens.'], [0, 'The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell.'], [1, 'Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though.'], [1, 'But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned?'], [1, 'That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together.'], [1, 'I did like how the book unveiled the Trisolaran culture through the game.'], [1, \"It was a smart way to build empathy with them and also understand what they've gone through across so many centuries.\"], [1, 'And who know a 3 body problem was an unsolvable math problem?'], [1, \"But I still don't get who made the game - maybe that will come in the next book.\"], [1, 'I loved this quote:'], [1, '\"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists?'], [1, 'How many neutrons and electrons?'], [1, 'Probably no fewer than a hundred million.'], [1, 'Every collision was probably the end of the civilizations and intelligences in a microcosmos.'], [1, 'In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons.'], [1, 'Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\"']], 'rating': 5, 'has_spoiler': True, 'book_id': '18245960', 'review_id': 'dfdbb7b0eb5a7e4c26d59a937e2e5feb'}\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "\n",
    "# Path to the .json.gz file\n",
    "file_path = 'goodreads_reviews_spoiler.json.gz'\n",
    "\n",
    "# Open and read the file line by line\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        # Parse each line as a separate JSON object\n",
    "        try:\n",
    "            review = json.loads(line)\n",
    "            data.append(review)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {e}\")\n",
    "\n",
    "# Example: Print the first parsed review\n",
    "if data:\n",
    "    print(data[0])\n",
    "else:\n",
    "    print(\"No data was parsed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6093c95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tylerlo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 132997467\n",
      "Unique Words: 761873\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Loop through each entry in the data\n",
    "for entry in data:\n",
    "    # Access the 'review_sentences' field\n",
    "    for _, sentence in entry['review_sentences']:\n",
    "        # Remove punctuation\n",
    "        sentence_cleaned = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Split into words and convert to lowercase\n",
    "        words = sentence_cleaned.lower().split()\n",
    "        # Stem and filter out stopwords\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                total_words += 1\n",
    "                word_count[stemmed_word] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Total Words:\", total_words)\n",
    "print(\"Unique Words:\", len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e715991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words by Frequency:\n",
      "book: 3132365\n",
      "read: 1624647\n",
      "like: 1580719\n",
      "love: 1411506\n",
      "one: 1191420\n",
      "stori: 1187276\n",
      "charact: 1135727\n",
      "realli: 1050887\n",
      "get: 767997\n",
      "time: 724121\n"
     ]
    }
   ],
   "source": [
    "# Build a vector of word counts as (count, word) tuples\n",
    "word_vector = [(count, word) for word, count in word_count.items()]\n",
    "\n",
    "# Sort the vector by word count in descending order\n",
    "word_vector = sorted(word_vector, reverse=True, key=lambda x: x[0])\n",
    "\n",
    "# Display the top 10 words\n",
    "print(\"Top 10 Words by Frequency:\")\n",
    "for count, word in word_vector[:10]:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6f7bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words:\n",
      "['book', 'read', 'like', 'love', 'one', 'stori', 'charact', 'realli', 'get', 'time']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of top words to extract\n",
    "nWords = 2000\n",
    "\n",
    "# Extract the top nWords\n",
    "top_words = [c[1] for c in word_vector[:nWords]]  # Get the words only\n",
    "wordId = dict(zip(top_words, range(nWords)))\n",
    "wordSet = set(top_words)\n",
    "# Print the first 10 top words for verification\n",
    "print(\"Top 10 Words:\")\n",
    "print(top_words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e5dfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def feature(datum):\n",
    "    bow = [0] * len(top_words)  # Initialize bag of words vector\n",
    "    \n",
    "    # Combine all sentences in 'review_sentences' into a single text\n",
    "    sentences = [s[1] for s in datum['review_sentences']]\n",
    "    r = ' '.join(sentences).lower()  # Combine sentences and convert to lowercase\n",
    "    \n",
    "    # Remove punctuation\n",
    "    r = ''.join(c for c in r if c not in string.punctuation)\n",
    "    \n",
    "    # Split into words and count only words in the top_words\n",
    "    words = r.split()\n",
    "    for w in words:\n",
    "        if w in wordSet:  # Only include top words\n",
    "            bow[wordId[w]] += 1\n",
    "    \n",
    "    return bow + [1]  # Add a bias term\n",
    "\n",
    "# Prepare features (X) and target values (y)\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['has_spoiler'] for d in data]  # Use 'has_spoiler' as the target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50d25d15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f53520e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = linear_model.Ridge(alpha=1, fit_intercept=False)  # Reduced regularization\n",
    "clf.fit(X, y)\n",
    "  # MSE + L2 regularization\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "weights = list(zip(top_words + [\"offset\"], theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca163e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', -0.0008230817502749303),\n",
       " ('read', -0.0033542069754568626),\n",
       " ('like', 0.0022223720651001586),\n",
       " ('love', -0.0002292461367788715),\n",
       " ('one', -0.001464389058951634),\n",
       " ('stori', -0.05896772528646231),\n",
       " ('charact', -0.09004035890667489),\n",
       " ('realli', 0.0),\n",
       " ('get', -0.0004563625436604259),\n",
       " ('time', -0.0006945155774822217)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b643d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc81b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
