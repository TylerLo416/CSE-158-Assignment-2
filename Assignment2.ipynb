{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f5c7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To upload to github, all changes you make to a clone of this repo / notebook should be stored somewhere and you can just push in a terminal or github desktop'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To upload to github, all changes you make to a clone of this repo / notebook \n",
    "should be stored somewhere and you can just push in a terminal or github desktop'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Everything here is made for the bag of words model he showed in Tuesday Nov 19 lecture \n",
    "- feel free to change anything, make your own model here or in a different ipynb'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5acab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': '8842281e1d1347389f2ab93d60773d4d', 'timestamp': '2017-08-30', 'review_sentences': [[0, 'This is a special book.'], [0, 'It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind.'], [0, 'This is what I love about good science fiction - it pushes your thinking about where things can go.'], [0, \"It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I've read.\"], [0, 'For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc.'], [0, 'It is a book about science, and aliens.'], [0, 'The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell.'], [1, 'Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though.'], [1, 'But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned?'], [1, 'That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together.'], [1, 'I did like how the book unveiled the Trisolaran culture through the game.'], [1, \"It was a smart way to build empathy with them and also understand what they've gone through across so many centuries.\"], [1, 'And who know a 3 body problem was an unsolvable math problem?'], [1, \"But I still don't get who made the game - maybe that will come in the next book.\"], [1, 'I loved this quote:'], [1, '\"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists?'], [1, 'How many neutrons and electrons?'], [1, 'Probably no fewer than a hundred million.'], [1, 'Every collision was probably the end of the civilizations and intelligences in a microcosmos.'], [1, 'In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons.'], [1, 'Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\"']], 'rating': 5, 'has_spoiler': True, 'book_id': '18245960', 'review_id': 'dfdbb7b0eb5a7e4c26d59a937e2e5feb'}\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "\n",
    "# Path to the .json.gz file\n",
    "file_path = 'goodreads_reviews_spoiler.json.gz'\n",
    "\n",
    "# Open and read the file line by line\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        # Parse each line as a separate JSON object\n",
    "        try:\n",
    "            review = json.loads(line)\n",
    "            data.append(review)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding line: {e}\")\n",
    "\n",
    "# Example: Print the first parsed review\n",
    "if data:\n",
    "    print(data[0])\n",
    "else:\n",
    "    print(\"No data was parsed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ef9363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tylerlo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 132997467\n",
      "Unique Words: 761873\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Loop through each entry in the data\n",
    "for entry in data:\n",
    "    # Access the 'review_sentences' field\n",
    "    for _, sentence in entry['review_sentences']:\n",
    "        # Remove punctuation\n",
    "        sentence_cleaned = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Split into words and convert to lowercase\n",
    "        words = sentence_cleaned.lower().split()\n",
    "        # Stem and filter out stopwords\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                total_words += 1\n",
    "                word_count[stemmed_word] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Total Words:\", total_words)\n",
    "print(\"Unique Words:\", len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bab2910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import classification_report, accuracy_score\\nimport nltk\\nimport string\\n\\n# Sample dataset\\ndata = [\\n    {\\n        \\'review_sentences\\': [\\n            [0, \\'This book is amazing.\\'],\\n            [1, \\'The ending where the hero dies is shocking.\\']\\n        ],\\n        \\'has_spoiler\\': True\\n    },\\n    {\\n        \\'review_sentences\\': [\\n            [0, \\'I loved the characters and the plot twists.\\'],\\n            [0, \\'A great sci-fi adventure.\\']\\n        ],\\n        \\'has_spoiler\\': False\\n    }\\n]\\n\\n# Step 1: Preprocess data\\ndef preprocess_review_sentences(data):\\n    reviews = []\\n    labels = []\\n    for entry in data:\\n        # Combine all sentences into one review\\n        review = \\' \\'.join(sentence[1] for sentence in entry[\\'review_sentences\\'])\\n        # Append to dataset\\n        reviews.append(review)\\n        labels.append(entry[\\'has_spoiler\\'])\\n    return reviews, labels\\n\\nreviews, labels = preprocess_review_sentences(data)\\n\\n# Step 2: Convert text to Bag of Words\\nvectorizer = CountVectorizer(lowercase=True, stop_words=\\'english\\', max_features=1000)  # Limit to 1000 features for simplicity\\nX = vectorizer.fit_transform(reviews)\\ny = labels\\n\\n# Step 3: Train/test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\\n\\n# Step 4: Train a classifier\\nmodel = LogisticRegression(random_state=42)\\nmodel.fit(X_train, y_train)\\n\\n# Step 5: Evaluate the model\\ny_pred = model.predict(X_test)\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\nprint(\"Classification Report:\")\\nprint(classification_report(y_test, y_pred))\\n\\n# Step 6: Inspect the most important words for spoilers\\ncoefficients = model.coef_[0]\\nfeature_names = vectorizer.get_feature_names_out()\\nimportant_words = sorted(zip(coefficients, feature_names), reverse=True, key=lambda x: abs(x[0]))\\n\\nprint(\"\\nTop 10 words most predictive of spoilers:\")\\nfor coef, word in important_words[:10]:\\n    print(f\"{word}: {coef:.4f}\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    {\n",
    "        'review_sentences': [\n",
    "            [0, 'This book is amazing.'],\n",
    "            [1, 'The ending where the hero dies is shocking.']\n",
    "        ],\n",
    "        'has_spoiler': True\n",
    "    },\n",
    "    {\n",
    "        'review_sentences': [\n",
    "            [0, 'I loved the characters and the plot twists.'],\n",
    "            [0, 'A great sci-fi adventure.']\n",
    "        ],\n",
    "        'has_spoiler': False\n",
    "    }\n",
    "]\n",
    "\n",
    "# Step 1: Preprocess data\n",
    "def preprocess_review_sentences(data):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for entry in data:\n",
    "        # Combine all sentences into one review\n",
    "        review = ' '.join(sentence[1] for sentence in entry['review_sentences'])\n",
    "        # Append to dataset\n",
    "        reviews.append(review)\n",
    "        labels.append(entry['has_spoiler'])\n",
    "    return reviews, labels\n",
    "\n",
    "reviews, labels = preprocess_review_sentences(data)\n",
    "\n",
    "# Step 2: Convert text to Bag of Words\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english', max_features=1000)  # Limit to 1000 features for simplicity\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "y = labels\n",
    "\n",
    "# Step 3: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Step 4: Train a classifier\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 6: Inspect the most important words for spoilers\n",
    "coefficients = model.coef_[0]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "important_words = sorted(zip(coefficients, feature_names), reverse=True, key=lambda x: abs(x[0]))\n",
    "\n",
    "print(\"\\nTop 10 words most predictive of spoilers:\")\n",
    "for coef, word in important_words[:10]:\n",
    "    print(f\"{word}: {coef:.4f}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79513d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61051ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
